{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logic_Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YDHui/Tensorflow_Study/blob/master/Logic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCDxo7YAefAl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "c5624428-10b5-41fe-aabf-bc4f1bea0e58"
      },
      "source": [
        "max_num_checkpoint = 10\n",
        "num_classes = 2\n",
        "batch_size = 512\n",
        "num_epochs = 10\n",
        "initial_learning_rate = 0.001\n",
        "learning_rate_decay_factor = 0.95\n",
        "num_epochs_per_decay = 1\n",
        "is_training = False\n",
        "fine_tuning = False\n",
        "online_test = True\n",
        "allow_soft_placement = True\n",
        "log_device_placement = False\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", reshape=True, one_hot=False)\n",
        "data={}\n",
        "data['train/image'] = mnist.train.images\n",
        "data['train/label'] = mnist.train.labels\n",
        "data['test/image'] = mnist.test.images\n",
        "data['test/label'] = mnist.test.labels\n",
        "index_list_train = []\n",
        "for sample_index in range(data['train/label'].shape[0]):\n",
        "    label = data['train/label'][sample_index]\n",
        "    if label == 1 or label == 0:\n",
        "        index_list_train.append(sample_index)\n",
        "index_list_test = []\n",
        "for sample_index in range(data['test/label'].shape[0]):\n",
        "    label = data['test/label'][sample_index]\n",
        "    if label == 1 or label == 0:\n",
        "        index_list_test.append(sample_index)\n",
        "data['train/image'] = mnist.train.images[index_list_train]\n",
        "data['train/label'] = mnist.train.labels[index_list_train]\n",
        "data['test/image'] = mnist.test.images[index_list_test]\n",
        "data['test/label'] = mnist.test.labels[index_list_test]\n",
        "dimensionality_train = data['train/image'].shape\n",
        "num_train_samples = dimensionality_train[0]\n",
        "num_features = dimensionality_train[1]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eyxRNeMhfuI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "53c113c2-1000-4601-ee87-0d2a7f9b7ea9"
      },
      "source": [
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
        "    decay_steps = int(num_train_samples / batch_size * num_epochs_per_decay)\n",
        "    learning_rate = tf.train.exponential_decay(initial_learning_rate,\n",
        "                                               global_step,\n",
        "                                               decay_steps,\n",
        "                                               learning_rate_decay_factor,\n",
        "                                               staircase=True,\n",
        "                                               name='exponential_decay_learning_rate')\n",
        "    image_place = tf.placeholder(tf.float32, shape=([None, num_features]), name='image')\n",
        "    label_place = tf.placeholder(tf.int32, shape=([None,]), name='gt')\n",
        "    label_one_hot = tf.one_hot(label_place, depth=num_classes, axis=-1)\n",
        "    dropout_param = tf.placeholder(tf.float32)\n",
        "    logits = tf.contrib.layers.fully_connected(inputs=image_place, num_outputs = num_classes, scope='fc')\n",
        "    with tf.name_scope('loss'):\n",
        "        loss_tensor = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label_one_hot))\n",
        "        prediction_correct = tf.equal(tf.argmax(logits, 1), tf.argmax(label_one_hot, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(prediction_correct, tf.float32))\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "    with tf.name_scope('train_op'):\n",
        "        gradients_and_variables = optimizer.compute_gradients(loss_tensor)\n",
        "        train_op = optimizer.apply_gradients(gradients_and_variables, global_step=global_step)\n",
        "    session_conf = tf.ConfigProto(\n",
        "        allow_soft_placement=allow_soft_placement,\n",
        "        log_device_placement=log_device_placement)\n",
        "    sess = tf.Session(graph=graph, config=session_conf)\n",
        "    with sess.as_default():\n",
        "\n",
        "        # The saver op.\n",
        "        saver = tf.train.Saver()\n",
        "\n",
        "        # Initialize all variables\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # The prefix for checkpoint files\n",
        "        checkpoint_prefix = 'model'\n",
        "\n",
        "        # If fie-tuning flag in 'True' the model will be restored.\n",
        "        if fine_tuning:\n",
        "            saver.restore(sess, os.path.join(checkpoint_path, checkpoint_prefix))\n",
        "            print(\"Model restored for fine-tuning...\")\n",
        "        test_accuracy = 0\n",
        "        for epoch in range(num_epochs):\n",
        "            total_batch_training = int(data['train/image'].shape[0] / batch_size)\n",
        "\n",
        "            # go through the batches\n",
        "            for batch_num in range(total_batch_training):\n",
        "                #################################################\n",
        "                ########## Get the training batches #############\n",
        "                #################################################\n",
        "\n",
        "                start_idx = batch_num * batch_size\n",
        "                end_idx = (batch_num + 1) * batch_size\n",
        "\n",
        "                # Fit training using batch data\n",
        "                train_batch_data, train_batch_label = data['train/image'][start_idx:end_idx], data['train/label'][\n",
        "                                                                                             start_idx:end_idx]\n",
        "\n",
        "                ########################################\n",
        "                ########## Run the session #############\n",
        "                ########################################\n",
        "\n",
        "                # Run optimization op (backprop) and Calculate batch loss and accuracy\n",
        "                # When the tensor tensors['global_step'] is evaluated, it will be incremented by one.\n",
        "                batch_loss, _, training_step = sess.run(\n",
        "                    [loss_tensor, train_op,\n",
        "                     global_step],\n",
        "                    feed_dict={image_place: train_batch_data,\n",
        "                               label_place: train_batch_label,\n",
        "                               dropout_param: 0.5})\n",
        "            print(\"Epoch \" + str(epoch + 1) + \", Training Loss= \" + \\\n",
        "                  \"{:.5f}\".format(batch_loss))\n",
        "\n",
        "        ############################################################################\n",
        "        ########## Run the session for pur evaluation on the test data #############\n",
        "        ############################################################################\n",
        "\n",
        "        # Evaluation of the model\n",
        "        test_accuracy = 100 * sess.run(accuracy, feed_dict={\n",
        "            image_place: data['test/image'],\n",
        "            label_place: data['test/label'],\n",
        "            dropout_param: 1.})\n",
        "\n",
        "        print(\"Final Test Accuracy is %% %.2f\" % test_accuracy)                "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-8-d8d3e99dca10>:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch 1, Training Loss= 0.27870\n",
            "Epoch 2, Training Loss= 0.12115\n",
            "Epoch 3, Training Loss= 0.07586\n",
            "Epoch 4, Training Loss= 0.05603\n",
            "Epoch 5, Training Loss= 0.04468\n",
            "Epoch 6, Training Loss= 0.03721\n",
            "Epoch 7, Training Loss= 0.03190\n",
            "Epoch 8, Training Loss= 0.02794\n",
            "Epoch 9, Training Loss= 0.02488\n",
            "Epoch 10, Training Loss= 0.02246\n",
            "Final Test Accuracy is % 99.95\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}